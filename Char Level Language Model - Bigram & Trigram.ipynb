{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "622ec5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6aecb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6d2681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77bde335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i2x = {i+1: x for i, x in enumerate(list(string.ascii_lowercase))}\n",
    "i2x[0] = '.'\n",
    "x2i = {x:i for i, x in i2x.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efeff9",
   "metadata": {},
   "source": [
    "## Bigram\n",
    "### Experimenting with 1 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96769246",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for word in words[:1]:\n",
    "    word = ['.'] + list(word) + ['.']\n",
    "    for i in range(len(word)-1):\n",
    "        i1 = x2i[word[i]]\n",
    "        i2 = x2i[word[i+1]]\n",
    "        xs.append(i1)\n",
    "        ys.append(i2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ac9c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = F.one_hot(xs, num_classes=27).float()\n",
    "y_train = F.one_hot(ys, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6e529343",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(27,27, requires_grad=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f6c94f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = X_train @ W\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e4932b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3930, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)   # every row should sum up to 1\n",
    "probs = softmax(out)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()   # negative mean log likelihood\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7184b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam([W], lr=0.1)\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()\n",
    "\n",
    "##### Naive Gradient Descent #######\n",
    "# W.grad = None\n",
    "# loss.backward()\n",
    "# W.data -= 0.1*W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c654f",
   "metadata": {},
   "source": [
    "### Using all inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f3058a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n"
     ]
    }
   ],
   "source": [
    "# prepping the data\n",
    "xs = []\n",
    "ys = []\n",
    "for word in words:\n",
    "    word = ['.'] + list(word) + ['.']\n",
    "    for i in range(len(word)-1):\n",
    "        i1 = x2i[word[i]]\n",
    "        i2 = x2i[word[i+1]]\n",
    "        xs.append(i1)\n",
    "        ys.append(i2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "X_train = F.one_hot(xs, num_classes=27).float()\n",
    "y_train = F.one_hot(ys, num_classes=27).float()\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(27,27, requires_grad=True, generator=g)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "optim = torch.optim.Adam([W], lr=0.1)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ed215b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.497866153717041\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for _ in range(40):\n",
    "    \n",
    "    # forward pass\n",
    "    out = X_train @ W\n",
    "    probs = softmax(out)\n",
    "    loss = -probs[torch.arange(X_train.shape[0]), ys].log().mean() + 0.01*(W**2).mean()   # add a regulation term\n",
    "    \n",
    "    # backward pass\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f849a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "axx.\n",
      "minaynnnyles.\n",
      "kondoderaea.\n",
      "memumizarie.\n"
     ]
    }
   ],
   "source": [
    "# sampling form the model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for _ in range(5):\n",
    "    name = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        # use the trained weights to make predictions\n",
    "        x = F.one_hot(torch.tensor([i]), num_classes=27).float()\n",
    "        out = x @ W   \n",
    "        probs = softmax(out)\n",
    "        i = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        name.append(i2x[i])\n",
    "        if i == 0:\n",
    "            break\n",
    "    print(''.join(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247938db",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "48d65db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 54])\n"
     ]
    }
   ],
   "source": [
    "# prepping the data\n",
    "x1s = []\n",
    "x2s = []\n",
    "ys = []\n",
    "for word in words:\n",
    "    word = ['.'] + ['.'] + list(word) + ['.']\n",
    "    for i in range(len(word)-2):\n",
    "        i1 = x2i[word[i]]\n",
    "        i2 = x2i[word[i+1]]\n",
    "        i3 = x2i[word[i+2]]\n",
    "        x1s.append(i1)\n",
    "        x2s.append(i2)\n",
    "        ys.append(i3)\n",
    "        \n",
    "x1s = torch.tensor(x1s)\n",
    "x2s = torch.tensor(x2s)\n",
    "X1_train = F.one_hot(x1s, num_classes=27).float()\n",
    "X2_train = F.one_hot(x2s, num_classes=27).float()\n",
    "X_train = torch.column_stack((X1_train, X2_train))   # X_train shape: N x 54\n",
    "\n",
    "\n",
    "ys = torch.tensor(ys)\n",
    "y_train = F.one_hot(ys, num_classes=27).float()\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(54,27, requires_grad=True, generator=g)  # matrix shape: 54 x 27 -> outputs still have 27 classes\n",
    "softmax = nn.Softmax(dim=1)\n",
    "optim = torch.optim.Adam([W], lr=0.6)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8defa599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.242240905761719\n",
      "3.1926229000091553\n",
      "2.728522300720215\n",
      "2.5960476398468018\n",
      "2.5908966064453125\n",
      "2.6151044368743896\n",
      "2.6266603469848633\n",
      "2.6185386180877686\n",
      "2.6052286624908447\n",
      "2.587240219116211\n",
      "2.5617809295654297\n",
      "2.534749746322632\n",
      "2.5137593746185303\n",
      "2.500807762145996\n",
      "2.491429328918457\n",
      "2.4801177978515625\n",
      "2.465301513671875\n",
      "2.4496428966522217\n",
      "2.436999797821045\n",
      "2.4292776584625244\n",
      "2.424954891204834\n",
      "2.421311616897583\n",
      "2.4167377948760986\n",
      "2.4111599922180176\n",
      "2.405439853668213\n",
      "2.40049147605896\n",
      "2.3964712619781494\n",
      "2.3929433822631836\n",
      "2.3893401622772217\n",
      "2.3855104446411133\n",
      "2.381984233856201\n",
      "2.3793246746063232\n",
      "2.37713885307312\n",
      "2.374643564224243\n",
      "2.371906042098999\n",
      "2.3695249557495117\n",
      "2.367680788040161\n",
      "2.3661837577819824\n",
      "2.3648431301116943\n",
      "2.363551616668701\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for _ in range(40):\n",
    "    \n",
    "    # forward pass\n",
    "    out = X_train @ W\n",
    "    probs = softmax(out)\n",
    "    loss = -probs[torch.arange(X_train.shape[0]), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    \n",
    "    # backward pass\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b5c13e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "ays.\n",
      "minayloryeer.\n",
      "klini.\n",
      "hokad.\n",
      "menseny.\n",
      "rie.\n",
      "pahiilaia.\n",
      "elon.\n",
      "hamirierien.\n",
      "elyonn.\n",
      "ga.\n",
      "ta.\n",
      "celyn.\n",
      "ilan.\n",
      "lumioh.\n",
      "mije.\n",
      "ai.\n",
      "ea.\n",
      "jijanca.\n"
     ]
    }
   ],
   "source": [
    "# sampling form the model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for _ in range(20):\n",
    "    name = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while True:\n",
    "        x1 = F.one_hot(torch.tensor([i]), num_classes=27).float()\n",
    "        x2 = F.one_hot(torch.tensor([j]), num_classes=27).float()\n",
    "        x = torch.column_stack((x1, x2))  # shape: 1 x 54\n",
    "        out = x @ W   \n",
    "        probs = softmax(out)\n",
    "        k = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        name.append(i2x[k])\n",
    "        if k == 0:\n",
    "            break\n",
    "        else:\n",
    "            i = j\n",
    "            j = k\n",
    "    print(''.join(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca35161",
   "metadata": {},
   "source": [
    "Trigram seems to do slightly better but not so much lol!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
